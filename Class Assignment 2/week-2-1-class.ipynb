{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lede Algorithms 2018 Week 2 class 1 - Text analysis\n",
    "\n",
    "Before running this notebook you will need to install a few things:\n",
    "\n",
    "```\n",
    "pip3 install textblob\n",
    "python3 -m textblob.download_corpora\n",
    "pip3 install scipy\n",
    "pip3 install scikit-learn\n",
    "```\n",
    "\n",
    "For more text analysis goodness, check out Jonathan Soma's 2017 notebooks:\n",
    "- [TextBlob spaCy sklearn lemmas stems and vectorization](http://jonathansoma.com/lede/algorithms-2017/classes/text-analysis/textblob-spacy-sklearn-lemmas-stems-and-vectorization/)\n",
    "- [Counting and Stemming](http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c39fc1044f87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[0;31m# avoid flakes unused variable error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "# Import the packages we will be using\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some press releases to play around with. These were scraped from NJ Senator Menendez' site in 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1530"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = pd.read_csv('menendez-press-releases.csv')\n",
    "len(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://menendez.senate.gov/newsroom/press/rele...</td>\n",
       "      <td>Menendez Statement on Black History Month\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://menendez.senate.gov/newsroom/press/rele...</td>\n",
       "      <td>Menendez Praises Susan G. Komen For Reversing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://menendez.senate.gov/newsroom/press/rele...</td>\n",
       "      <td>Menendez Applauds Dentists’ Pro-Bono Work For ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://menendez.senate.gov/newsroom/press/rele...</td>\n",
       "      <td>Senator Menendez Applauds Passage of STOCK Act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://menendez.senate.gov/newsroom/press/rele...</td>\n",
       "      <td>Menendez Hails Banking Committee Passage of Ir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  http://menendez.senate.gov/newsroom/press/rele...   \n",
       "1  http://menendez.senate.gov/newsroom/press/rele...   \n",
       "2  http://menendez.senate.gov/newsroom/press/rele...   \n",
       "3  http://menendez.senate.gov/newsroom/press/rele...   \n",
       "4  http://menendez.senate.gov/newsroom/press/rele...   \n",
       "\n",
       "                                                text  \n",
       "0  Menendez Statement on Black History Month\\n   ...  \n",
       "1  Menendez Praises Susan G. Komen For Reversing ...  \n",
       "2  Menendez Applauds Dentists’ Pro-Bono Work For ...  \n",
       "3  Senator Menendez Applauds Passage of STOCK Act...  \n",
       "4  Menendez Hails Banking Committee Passage of Ir...  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "These press releases are on all sorts of topics. Take a look at a few, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senator Menendez Slams Unfair Imprisonment of Former Ukrainian Prime Minister Yulia Tymoshenko\n",
      "                    \n",
      "                      February 1, 2012\n",
      "                     WASHINGTON – United States Senator Robert Menendez (D-NJ) participated in the Senate Foreign Relations Committee hearing on Ukraine today, giving him the opportunity to meet Eugenia Tymoshenko and to discuss the inhumane detention of her mother, the former Prime Minister Yulia Tymoshenko.  Last October, a Ukrainian court sentenced Yulia Tymoshenko to seven years in prison after she was found guilty of abuse of office when brokering a 2009 gas deal with Russia.  The Senator expressed his sympathy and support to Ms. Tymoshenko and vowed to assist in her efforts to have her mother freed from prison.\n",
      "\n",
      " “Your mother is a pioneering and incredibly strong woman,” the Senator told the younger Tymoshenko. “Yulia is an example for all people who care so much about their country that they are willing to endure extraordinary hardship and not just lay down in the face of oppression and cruelty. I think this hearing is a wonderful way to inform the American people not only about your mother, but the other opposition leaders in jail and to keep the pressure on the Ukrainian authorities to give Yulia her freedom back.”\n",
      " “Every day the human rights situation in Ukraine worsens and it’s starting to remind me of the shameful conditions of the Soviet era.  Yulia Tymoshenko worked very hard to throw off the tyranny of the Soviet past, and to see her and other opposition leaders in jail cells is a reminder of how much work remains to be done to improve human rights and political freedom in today’s Ukraine.  We must convince President Yanukovych that Ukraine’s path to freedom and economic prosperity is not through Soviet-style centralized government, but by way of releasing the power, intelligence and dignity of the Ukrainian people while respecting the rights of all its citizens,” added Menendez.\n",
      "\n",
      "\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "print(pr.text[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are press releases about government programs, holidays, foreign policy, and more. Can an algorithm tell us something about which topics there are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sentences and tokens\n",
    "\n",
    "The first step of text analysis is typically breaking the text into sentences and words or more accurately, \"tokens\" which are basically words but can also be punctuation and numbers.\n",
    "\n",
    "We'll use the TextBlob package, which has many easy and useful text processing methods -- though as we will see it's also kind of dumb in many cases.\n",
    "\n",
    "Let's start by trying analyze the sentences of the first press release in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Menendez and Lautenberg Applaud USDOT’s $4.3 Million Award to National Transit Institute at Rutgers\n",
       "                     \n",
       "                             Funding will provide job training and education for public transportation workers\n",
       "                     \n",
       "                       August 3, 2011\n",
       "                      WASHINGTON, D.C. – Today, U.S.\"),\n",
       " Sentence(\"Senators Robert Menendez (D-NJ) and Frank R. Lautenberg (D-NJ) applauded Secretary Ray LaHood on a $4.3 million grant from the U.S. Department of Transportation in support of the National Transit Institute (NTI) at Rutgers.\"),\n",
       " Sentence(\"Since 1991, NTI has served as the premiere research, training, and educational institute in the country dedicated to public transportation.\"),\n",
       " Sentence(\"This award will enable critical enhancements to NTI’s ongoing work, including safety and security training, procurement, planning and advanced technologies.\"),\n",
       " Sentence(\"“Transit is a critical element of our transportation network and recognition of its importance continues to rise,” said Menendez.\"),\n",
       " Sentence(\"“Today, with gas prices around $4 a gallon and oil companies reaping record profits, with the threat of climate change and growing wealth disparity, transit is part of the solution for a number of interconnected challenges.\"),\n",
       " Sentence(\"NTI’s work is essential for the industry to continue to create good, long term jobs, provide families with access to opportunity, and help our communities grow in ways that are smart and efficient.”\n",
       " \"Millions of people count on efficient and reliable public transportation to get to and from their homes and their jobs, and transit workers make this possible,\" said Senator Lautenberg.\"),\n",
       " Sentence(\"\"NTI at Rutgers provides critical training for transit operators around the country and helps ensure the best management, safety and security of our public transportation systems.\"),\n",
       " Sentence(\"This federal grant makes an investment in NTI so that it can continue its work to keep Americans on the move.\"\"),\n",
       " Sentence(\"“This grant will ensure that the existing and importantly, the new generation of public transportation workers receive the training that they need to comply with federal regulations and operate safe and efficient transit services,” said NTI’s director, Paul Larrousse.NTI works cooperatively through partnerships with industry, government, institutions, and associations to develop high quality programs that build careers and help make our communities healthier and more livable.\"),\n",
       " Sentence(\"###\")]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "press_release_text = pr.text[212]\n",
    "doc = TextBlob(press_release_text)\n",
    "doc.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, TextBlob is a little naive about what counts as a sentence. Or perhaps the problem is that real text contains lots of things that aren't really sentences. What should we do with the title and the dateline? Even so, it seems to have problems with quotes and newlines.\n",
    "\n",
    "Anyway, let's take one of these sentences and play with it further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence(\"“This grant will ensure that the existing and importantly, the new generation of public transportation workers receive the training that they need to comply with federal regulations and operate safe and efficient transit services,” said NTI’s director, Paul Larrousse.NTI works cooperatively through partnerships with industry, government, institutions, and associations to develop high quality programs that build careers and help make our communities healthier and more livable.\")"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = doc.sentences[9]\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `Sentence` object acts just like a Python string. Let's try to break it into words for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['“This', 'grant', 'will', 'ensure', 'that', 'the', 'existing', 'and', 'importantly,', 'the', 'new', 'generation', 'of', 'public', 'transportation', 'workers', 'receive', 'the', 'training', 'that', 'they', 'need', 'to', 'comply', 'with', 'federal', 'regulations', 'and', 'operate', 'safe', 'and', 'efficient', 'transit', 'services,”', 'said', 'NTI’s', 'director,', 'Paul', 'Larrousse.NTI', 'works', 'cooperatively', 'through', 'partnerships', 'with', 'industry,', 'government,', 'institutions,', 'and', 'associations', 'to', 'develop', 'high', 'quality', 'programs', 'that', 'build', 'careers', 'and', 'help', 'make', 'our', 'communities', 'healthier', 'and', 'more', 'livable.'])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all of the punctuation and capitalization is still there. If we're counting occurences of the word \"nation\" we will miss \"Nation’s\". We need a smarter way to extract words. This process is called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['“', 'This', 'grant', 'will', 'ensure', 'that', 'the', 'existing', 'and', 'importantly', ',', 'the', 'new', 'generation', 'of', 'public', 'transportation', 'workers', 'receive', 'the', 'training', 'that', 'they', 'need', 'to', 'comply', 'with', 'federal', 'regulations', 'and', 'operate', 'safe', 'and', 'efficient', 'transit', 'services', ',', '”', 'said', 'NTI', '’', 's', 'director', ',', 'Paul', 'Larrousse.NTI', 'works', 'cooperatively', 'through', 'partnerships', 'with', 'industry', ',', 'government', ',', 'institutions', ',', 'and', 'associations', 'to', 'develop', 'high', 'quality', 'programs', 'that', 'build', 'careers', 'and', 'help', 'make', 'our', 'communities', 'healthier', 'and', 'more', 'livable', '.'])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what about different forms of the same word? Suppose we want to count \"education\" and \"educate\" as the same thing? This is where _lemmatization_ and _stemming_ come in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL: “ | LEMMA: “ | STEM: “\n",
      "ORIGINAL: This | LEMMA: This | STEM: thi\n",
      "ORIGINAL: grant | LEMMA: grant | STEM: grant\n",
      "ORIGINAL: will | LEMMA: will | STEM: will\n",
      "ORIGINAL: ensure | LEMMA: ensure | STEM: ensur\n",
      "ORIGINAL: that | LEMMA: that | STEM: that\n",
      "ORIGINAL: the | LEMMA: the | STEM: the\n",
      "ORIGINAL: existing | LEMMA: existing | STEM: exist\n",
      "ORIGINAL: and | LEMMA: and | STEM: and\n",
      "ORIGINAL: importantly | LEMMA: importantly | STEM: importantli\n",
      "ORIGINAL: the | LEMMA: the | STEM: the\n",
      "ORIGINAL: new | LEMMA: new | STEM: new\n",
      "ORIGINAL: generation | LEMMA: generation | STEM: gener\n",
      "ORIGINAL: of | LEMMA: of | STEM: of\n",
      "ORIGINAL: public | LEMMA: public | STEM: public\n",
      "ORIGINAL: transportation | LEMMA: transportation | STEM: transport\n",
      "ORIGINAL: workers | LEMMA: worker | STEM: worker\n",
      "ORIGINAL: receive | LEMMA: receive | STEM: receiv\n",
      "ORIGINAL: the | LEMMA: the | STEM: the\n",
      "ORIGINAL: training | LEMMA: training | STEM: train\n",
      "ORIGINAL: that | LEMMA: that | STEM: that\n",
      "ORIGINAL: they | LEMMA: they | STEM: they\n",
      "ORIGINAL: need | LEMMA: need | STEM: need\n",
      "ORIGINAL: to | LEMMA: to | STEM: to\n",
      "ORIGINAL: comply | LEMMA: comply | STEM: compli\n",
      "ORIGINAL: with | LEMMA: with | STEM: with\n",
      "ORIGINAL: federal | LEMMA: federal | STEM: feder\n",
      "ORIGINAL: regulations | LEMMA: regulation | STEM: regul\n",
      "ORIGINAL: and | LEMMA: and | STEM: and\n",
      "ORIGINAL: operate | LEMMA: operate | STEM: oper\n",
      "ORIGINAL: safe | LEMMA: safe | STEM: safe\n",
      "ORIGINAL: and | LEMMA: and | STEM: and\n",
      "ORIGINAL: efficient | LEMMA: efficient | STEM: effici\n",
      "ORIGINAL: transit | LEMMA: transit | STEM: transit\n",
      "ORIGINAL: services | LEMMA: service | STEM: servic\n",
      "ORIGINAL: ” | LEMMA: ” | STEM: ”\n",
      "ORIGINAL: said | LEMMA: said | STEM: said\n",
      "ORIGINAL: NTI | LEMMA: NTI | STEM: nti\n",
      "ORIGINAL: ’ | LEMMA: ’ | STEM: ’\n",
      "ORIGINAL: s | LEMMA: s | STEM: s\n",
      "ORIGINAL: director | LEMMA: director | STEM: director\n",
      "ORIGINAL: Paul | LEMMA: Paul | STEM: paul\n",
      "ORIGINAL: Larrousse.NTI | LEMMA: Larrousse.NTI | STEM: larrousse.nti\n",
      "ORIGINAL: works | LEMMA: work | STEM: work\n",
      "ORIGINAL: cooperatively | LEMMA: cooperatively | STEM: cooper\n",
      "ORIGINAL: through | LEMMA: through | STEM: through\n",
      "ORIGINAL: partnerships | LEMMA: partnership | STEM: partnership\n",
      "ORIGINAL: with | LEMMA: with | STEM: with\n",
      "ORIGINAL: industry | LEMMA: industry | STEM: industri\n",
      "ORIGINAL: government | LEMMA: government | STEM: govern\n",
      "ORIGINAL: institutions | LEMMA: institution | STEM: institut\n",
      "ORIGINAL: and | LEMMA: and | STEM: and\n",
      "ORIGINAL: associations | LEMMA: association | STEM: associ\n",
      "ORIGINAL: to | LEMMA: to | STEM: to\n",
      "ORIGINAL: develop | LEMMA: develop | STEM: develop\n",
      "ORIGINAL: high | LEMMA: high | STEM: high\n",
      "ORIGINAL: quality | LEMMA: quality | STEM: qualiti\n",
      "ORIGINAL: programs | LEMMA: program | STEM: program\n",
      "ORIGINAL: that | LEMMA: that | STEM: that\n",
      "ORIGINAL: build | LEMMA: build | STEM: build\n",
      "ORIGINAL: careers | LEMMA: career | STEM: career\n",
      "ORIGINAL: and | LEMMA: and | STEM: and\n",
      "ORIGINAL: help | LEMMA: help | STEM: help\n",
      "ORIGINAL: make | LEMMA: make | STEM: make\n",
      "ORIGINAL: our | LEMMA: our | STEM: our\n",
      "ORIGINAL: communities | LEMMA: community | STEM: commun\n",
      "ORIGINAL: healthier | LEMMA: healthier | STEM: healthier\n",
      "ORIGINAL: and | LEMMA: and | STEM: and\n",
      "ORIGINAL: more | LEMMA: more | STEM: more\n",
      "ORIGINAL: livable | LEMMA: livable | STEM: livabl\n"
     ]
    }
   ],
   "source": [
    "for word in s.words:\n",
    "    print(\"ORIGINAL:\", word, \"| LEMMA:\", word.lemmatize(), \"| STEM:\", word.stem())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example `lemmatize()` mostly just removes pluralization. `stem()` goes further and produces many non-words like \"develop\". It's just a set of rules that try to parse out English morphology. The rule set here is called [Porter stemming](https://snowballstem.org/algorithms/porter/stemmer.html), and there are similar algorithms for [many languages](https://snowballstem.org/algorithms/). Lemmatization is actually smarter because it uses a dictionary, so it can undo common verb inflections... if you tell TextBlob that the word is a verb (to be fair, it can [figure out parts of speech](https://textblob.readthedocs.io/en/dev/quickstart.html#part-of-speech-tagging) if you ask it to.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"running\").words[0].lemmatize('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"ran\").words[0].lemmatize('v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes we're going to use TexBlob to make a basic tokenizing function that just makes everything lowercase and throws token with less than 3 characters, which throws out punctuation tokens too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    blob = TextBlob(s.lower())\n",
    "    words = [token for token in blob.words if len(token)>2]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document similarity\n",
    "One of the main things we want to do with documents is compare them to each other. This is useful for many things:\n",
    "- matching search queries against documents\n",
    "- classifying documents (think of this as sorting them into piles)\n",
    "- clustering documents by topic\n",
    "\n",
    "To do this, we need to write a function that takes two documents and returns a number representing some concept of \"similarity\" that will be useful for these tasks. To do _that_, we're first going to turn each document into a vector. Our first attempt at this will be by counting the number of tokens of each kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc2vec_count(s):\n",
    "    tokens = tokenize(s)\n",
    "    vec = {}\n",
    "    for t in tokens:\n",
    "        vec[t] = vec.get(t, 0) + 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 1, 'cat': 1, 'mat': 1, 'the': 2}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_count(\"the cat and the mat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to compare two word count vectors is to count the number of overlapping words. Each word can appear more than once, so we'll multiply together the counts of the same word in each docmument. Why? Because this leads us to a Euclidian feature vector with natural geometric properties, which makes it possible to think about wbat is going on with spatial analogies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_similarity(a_vec,b_vec):\n",
    "    total = 0\n",
    "    for word in a_vec:\n",
    "        if word in b_vec:\n",
    "            total += a_vec[word]*b_vec[word]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = doc2vec_count(str(doc.sentences[6]))  # need str to convert Sentence object to string\n",
    "b = doc2vec_count(str(doc.sentences[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nti': 1, 'work': 1, 'essential': 1, 'for': 1, 'the': 1, 'industry': 1, 'continue': 1, 'create': 1, 'good': 1, 'long': 1, 'term': 1, 'jobs': 2, 'provide': 1, 'families': 1, 'with': 1, 'access': 1, 'opportunity': 1, 'and': 6, 'help': 1, 'our': 1, 'communities': 1, 'grow': 1, 'ways': 1, 'that': 1, 'are': 1, 'smart': 1, 'efficient': 2, 'millions': 1, 'people': 1, 'count': 1, 'reliable': 1, 'public': 1, 'transportation': 1, 'get': 1, 'from': 1, 'their': 2, 'homes': 1, 'transit': 1, 'workers': 1, 'make': 1, 'this': 1, 'possible': 1, 'said': 1, 'senator': 1, 'lautenberg': 1}\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 1, 'grant': 1, 'will': 1, 'ensure': 1, 'that': 3, 'the': 3, 'existing': 1, 'and': 6, 'importantly': 1, 'new': 1, 'generation': 1, 'public': 1, 'transportation': 1, 'workers': 1, 'receive': 1, 'training': 1, 'they': 1, 'need': 1, 'comply': 1, 'with': 2, 'federal': 1, 'regulations': 1, 'operate': 1, 'safe': 1, 'efficient': 1, 'transit': 1, 'services': 1, 'said': 1, 'nti': 1, 'director': 1, 'paul': 1, 'larrousse.nti': 1, 'works': 1, 'cooperatively': 1, 'through': 1, 'partnerships': 1, 'industry': 1, 'government': 1, 'institutions': 1, 'associations': 1, 'develop': 1, 'high': 1, 'quality': 1, 'programs': 1, 'build': 1, 'careers': 1, 'help': 1, 'make': 1, 'our': 1, 'communities': 1, 'healthier': 1, 'more': 1, 'livable': 1}\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_similarity(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, but what does \"58\" mean? One problem we are going to have is that longer documents will tend to be more similar to everything else. More words mean more words can match. We will solve this problem by normalizing each document vector so that it has length 1, meaning that the sum of the _squares_ of the elements is one -- this is Pyhagoras, so we can think of a document as a unit vector now, or a direction, in a space that has as many dimensions as the vocabulary size. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc2vec_normalized(s):\n",
    "    tokens = tokenize(s)\n",
    "    vec = {}\n",
    "    for t in tokens:\n",
    "        vec[t] = vec.get(t, 0) + 1 # get from dict with a default of 0 if missing\n",
    "        \n",
    "    length = math.sqrt(sum([x*x for x in vec.values()]))  # length of a vector, according to Pythagoras\n",
    "    for word,value in vec.items():\n",
    "        vec[word] /= length\n",
    "        \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = doc2vec_normalized(str(doc.sentences[6]))  # need str to convert Sentence object to string\n",
    "b = doc2vec_normalized(str(doc.sentences[9]))\n",
    "c = doc2vec_normalized(str(doc.sentences[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'today': 0.1690308509457033, 'with': 0.3380617018914066, 'gas': 0.1690308509457033, 'prices': 0.1690308509457033, 'around': 0.1690308509457033, 'gallon': 0.1690308509457033, 'and': 0.3380617018914066, 'oil': 0.1690308509457033, 'companies': 0.1690308509457033, 'reaping': 0.1690308509457033, 'record': 0.1690308509457033, 'profits': 0.1690308509457033, 'the': 0.3380617018914066, 'threat': 0.1690308509457033, 'climate': 0.1690308509457033, 'change': 0.1690308509457033, 'growing': 0.1690308509457033, 'wealth': 0.1690308509457033, 'disparity': 0.1690308509457033, 'transit': 0.1690308509457033, 'part': 0.1690308509457033, 'solution': 0.1690308509457033, 'for': 0.1690308509457033, 'number': 0.1690308509457033, 'interconnected': 0.1690308509457033, 'challenges': 0.1690308509457033}\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our similarity function says that the first two sentences are the most similar, because they have words like \"industry\", \"efficient\", and \"transit\" in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5943484047696529\n",
      "0.37583907018239515\n",
      "0.32251021858460976\n"
     ]
    }
   ],
   "source": [
    "print(doc_similarity(a,b))\n",
    "print(doc_similarity(b,c))\n",
    "print(doc_similarity(a,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to look at document vectors is as list of top words. Let's sort document vectors by decreasing value to try to get an idea of what the entire press release document is \"about\", and print the top 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sorted_vector(v):\n",
    "    # this \"lambda\" thing is an anonymous function, google me to unluck bonus coding knowledge\n",
    "    sorted_list = sorted(v.items(), key=lambda x: (x[1],x[0]), reverse=True) \n",
    "    sorted_list = sorted_list[:20]\n",
    "    print('\\n'.join([str(x) for x in sorted_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('and', 0.6359987280038161)\n",
      "('their', 0.211999576001272)\n",
      "('jobs', 0.211999576001272)\n",
      "('efficient', 0.211999576001272)\n",
      "('workers', 0.105999788000636)\n",
      "('work', 0.105999788000636)\n",
      "('with', 0.105999788000636)\n",
      "('ways', 0.105999788000636)\n",
      "('transportation', 0.105999788000636)\n",
      "('transit', 0.105999788000636)\n",
      "('this', 0.105999788000636)\n",
      "('the', 0.105999788000636)\n",
      "('that', 0.105999788000636)\n",
      "('term', 0.105999788000636)\n",
      "('smart', 0.105999788000636)\n",
      "('senator', 0.105999788000636)\n",
      "('said', 0.105999788000636)\n",
      "('reliable', 0.105999788000636)\n",
      "('public', 0.105999788000636)\n",
      "('provide', 0.105999788000636)\n"
     ]
    }
   ],
   "source": [
    "print_sorted_vector(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('and', 0.6364382128493583)\n",
      "('the', 0.35972594639311556)\n",
      "('transportation', 0.19369858651936991)\n",
      "('transit', 0.19369858651936991)\n",
      "('nti', 0.19369858651936991)\n",
      "('with', 0.13835613322812138)\n",
      "('training', 0.13835613322812138)\n",
      "('that', 0.13835613322812138)\n",
      "('public', 0.13835613322812138)\n",
      "('this', 0.1106849065824971)\n",
      "('our', 0.1106849065824971)\n",
      "('for', 0.1106849065824971)\n",
      "('workers', 0.08301367993687282)\n",
      "('work', 0.08301367993687282)\n",
      "('will', 0.08301367993687282)\n",
      "('said', 0.08301367993687282)\n",
      "('rutgers', 0.08301367993687282)\n",
      "('menendez', 0.08301367993687282)\n",
      "('lautenberg', 0.08301367993687282)\n",
      "('institute', 0.08301367993687282)\n"
     ]
    }
   ],
   "source": [
    "print_sorted_vector(doc2vec_normalized(press_release_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad... but is this press release really \"about\" the words like \"the\" and \"and\"? We need something better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF weighting\n",
    "As we discussed in class, term frequency / inverse document frequency is a word weighting scheme that tries to give less weight to words that appear in many documents. This will solve our \"the\" problem, and it will also help drop out topic words that are common to the entire corpus. Rather than writing it ourselves, we're going to use the implementation in the `scikit` library.\n",
    "\n",
    "Scikit includes a bunch of built in vectorizers, such as classic counting. Let's turn the first ten press releases into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1,105,000</th>\n",
       "      <th>1,160,000</th>\n",
       "      <th>100</th>\n",
       "      <th>1099</th>\n",
       "      <th>13,381</th>\n",
       "      <th>142nd</th>\n",
       "      <th>15th</th>\n",
       "      <th>170</th>\n",
       "      <th>170,000</th>\n",
       "      <th>179,550</th>\n",
       "      <th>...</th>\n",
       "      <th>x-ray</th>\n",
       "      <th>yanukovych</th>\n",
       "      <th>year</th>\n",
       "      <th>yearli</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>yulia</th>\n",
       "      <th>–through</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1057 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1,105,000  1,160,000  100  1099  13,381  142nd  15th  170  170,000  \\\n",
       "0          0          0    2     0       0      1     1    0        0   \n",
       "1          0          0    0     0       0      0     0    0        1   \n",
       "2          0          0    1     0       0      0     0    0        0   \n",
       "3          0          0    0     0       0      0     0    0        0   \n",
       "4          1          1    0     0       0      0     0    0        0   \n",
       "5          0          0    0     0       0      0     0    0        0   \n",
       "6          0          0    0     0       1      0     0    0        0   \n",
       "7          0          0    1     1       0      0     0    1        0   \n",
       "8          0          0    0     0       0      0     0    0        0   \n",
       "9          0          0    0     0       0      0     0    0        0   \n",
       "\n",
       "   179,550    ...     x-ray  yanukovych  year  yearli  yesterday  york  young  \\\n",
       "0        0    ...         0           0     0       0          0     0      0   \n",
       "1        0    ...         0           0     2       0          1     0      0   \n",
       "2        0    ...         1           0     3       1          0     0      0   \n",
       "3        0    ...         0           0     0       0          0     0      0   \n",
       "4        0    ...         0           0     0       0          0     1      3   \n",
       "5        0    ...         0           0     6       0          0     0      0   \n",
       "6        1    ...         0           0     0       0          0     0      0   \n",
       "7        0    ...         0           0     4       0          0     0      0   \n",
       "8        0    ...         0           1     1       0          0     0      0   \n",
       "9        0    ...         0           0     0       0          0     0      0   \n",
       "\n",
       "   younger  yulia  –through  \n",
       "0        0      0         0  \n",
       "1        0      0         0  \n",
       "2        0      0         0  \n",
       "3        0      0         0  \n",
       "4        0      0         0  \n",
       "5        0      0         0  \n",
       "6        0      0         0  \n",
       "7        0      0         0  \n",
       "8        1      6         0  \n",
       "9        0      0         1  \n",
       "\n",
       "[10 rows x 1057 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a new Count Vectorizer!!!! It will conveniently remove stop words if we tell it what language we're using\n",
    "vectorizer = CountVectorizer(stop_words='english', tokenizer=tokenize)\n",
    "\n",
    "# Use the vectorizor we just made. The name fit_transform will be clearer later when we use it for machine learning\n",
    "matrix = vectorizer.fit_transform(pr.text[0:10])\n",
    "\n",
    "# The easiest way to see what happenned is to make a dataframe\n",
    "results = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row is a document, and each column corresponds to a single vocabulary word or token. And there are a lot of columns, which means we can think of these as points in 1,507 dimensional space.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,105,000',\n",
       " '1,160,000',\n",
       " '100',\n",
       " '1099',\n",
       " '13,381',\n",
       " '142nd',\n",
       " '15th',\n",
       " '170',\n",
       " '170,000',\n",
       " '179,550',\n",
       " '1903',\n",
       " '1980s',\n",
       " '1983',\n",
       " '1996',\n",
       " '1998',\n",
       " '20,000',\n",
       " '2008',\n",
       " '2009',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '265,950',\n",
       " '3.8',\n",
       " '315,000',\n",
       " '335',\n",
       " '350',\n",
       " '351,554',\n",
       " '4,600',\n",
       " '47,200',\n",
       " '5,000',\n",
       " '51.5m',\n",
       " '519',\n",
       " '5:00',\n",
       " '5million',\n",
       " '6,400',\n",
       " '6.5',\n",
       " '650,000',\n",
       " '65m',\n",
       " '7.8',\n",
       " '750,000',\n",
       " '770,000',\n",
       " '771',\n",
       " '929,088',\n",
       " '96-3',\n",
       " 'able',\n",
       " 'abuse',\n",
       " 'access',\n",
       " 'according',\n",
       " 'accountability',\n",
       " 'acquisition',\n",
       " 'act',\n",
       " 'action',\n",
       " 'add',\n",
       " 'added',\n",
       " 'additionally',\n",
       " 'address',\n",
       " 'addresses',\n",
       " 'adopted',\n",
       " 'advance',\n",
       " 'adversely',\n",
       " 'advocate',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affiliates',\n",
       " 'afford',\n",
       " 'afg',\n",
       " 'afin',\n",
       " 'african',\n",
       " 'african-american',\n",
       " 'african-americans',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agents',\n",
       " 'ahmadinejad',\n",
       " 'aid',\n",
       " 'air',\n",
       " 'alarm',\n",
       " 'ali',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'ambassador',\n",
       " 'amendment',\n",
       " 'amendments',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'anniversary',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'annual',\n",
       " 'ansar-e-hezbollah',\n",
       " 'apologize',\n",
       " 'apologized',\n",
       " 'applaud',\n",
       " 'applauded',\n",
       " 'applauds',\n",
       " 'applications',\n",
       " 'apply',\n",
       " 'appreciation',\n",
       " 'appropriations',\n",
       " 'approval',\n",
       " 'arabia',\n",
       " 'areas',\n",
       " 'aside',\n",
       " 'asked',\n",
       " 'assembly',\n",
       " 'assess',\n",
       " 'assets',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'assistants',\n",
       " 'association',\n",
       " 'atlantic',\n",
       " 'attach',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'attended',\n",
       " 'authorities',\n",
       " 'authority',\n",
       " 'authorization',\n",
       " 'available',\n",
       " 'average',\n",
       " 'awarded',\n",
       " 'bank',\n",
       " 'banking',\n",
       " 'banks',\n",
       " 'bank—all',\n",
       " 'baptist',\n",
       " 'barracks',\n",
       " 'bars',\n",
       " 'base',\n",
       " 'based',\n",
       " 'basij-e',\n",
       " 'beginning',\n",
       " 'beliefs',\n",
       " 'benefit',\n",
       " 'benefits',\n",
       " 'bergen/passaic',\n",
       " 'best',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'billion',\n",
       " 'bills',\n",
       " 'bionj',\n",
       " 'biotechnology',\n",
       " 'bipartisan',\n",
       " 'black',\n",
       " 'blueprint',\n",
       " 'bomb',\n",
       " 'bombing',\n",
       " 'bono',\n",
       " 'boon',\n",
       " 'boost',\n",
       " 'box',\n",
       " 'branch',\n",
       " 'brand',\n",
       " 'breast',\n",
       " 'brennan',\n",
       " 'brigadier',\n",
       " 'brinker',\n",
       " 'bro-bono',\n",
       " 'broader',\n",
       " 'brokering',\n",
       " 'brown',\n",
       " 'budget',\n",
       " 'built',\n",
       " 'business',\n",
       " 'businesses',\n",
       " 'buster',\n",
       " 'buy',\n",
       " 'bylaws',\n",
       " 'cabinet',\n",
       " 'called',\n",
       " 'cancer',\n",
       " 'cancers',\n",
       " 'capacity',\n",
       " 'capital',\n",
       " 'capitol',\n",
       " 'capture',\n",
       " 'care',\n",
       " 'career',\n",
       " 'cargo',\n",
       " 'cars',\n",
       " 'case',\n",
       " 'caused',\n",
       " 'cecile',\n",
       " 'celebrate',\n",
       " 'celebrated',\n",
       " 'celebration',\n",
       " 'cells',\n",
       " 'center',\n",
       " 'centers',\n",
       " 'central',\n",
       " 'centralized',\n",
       " 'certain',\n",
       " 'chair',\n",
       " 'chairman',\n",
       " 'championed',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'chapter',\n",
       " 'charge',\n",
       " 'cheaper',\n",
       " 'check-up',\n",
       " 'children',\n",
       " 'chip',\n",
       " 'choice',\n",
       " 'church',\n",
       " 'circumvent',\n",
       " 'cisada',\n",
       " 'citibank',\n",
       " 'citizens',\n",
       " 'city',\n",
       " 'clarifies',\n",
       " 'clean',\n",
       " 'cleaner',\n",
       " 'cleanings',\n",
       " 'clearinghouse',\n",
       " 'climate',\n",
       " 'clinic',\n",
       " 'clinical',\n",
       " 'clinics',\n",
       " 'closer',\n",
       " 'co-authored',\n",
       " 'co-sponsor',\n",
       " 'co-sponsored',\n",
       " 'coalition',\n",
       " 'colleagues',\n",
       " 'columbia',\n",
       " 'comes',\n",
       " 'commission',\n",
       " 'commitment',\n",
       " 'committed',\n",
       " 'committee',\n",
       " 'communications',\n",
       " 'communities',\n",
       " 'community',\n",
       " 'commuters',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'company/tanker',\n",
       " 'compels',\n",
       " 'compensate',\n",
       " 'competitive',\n",
       " 'comprehensive',\n",
       " 'concluded',\n",
       " 'concludes',\n",
       " 'condition',\n",
       " 'conditions',\n",
       " 'conducive',\n",
       " 'congestion',\n",
       " 'congress',\n",
       " 'congressional',\n",
       " 'consider',\n",
       " 'consolidation',\n",
       " 'container',\n",
       " 'containers',\n",
       " 'contains',\n",
       " 'continue',\n",
       " 'contracts',\n",
       " 'contributions',\n",
       " 'convince',\n",
       " 'cooperative',\n",
       " 'coordinate',\n",
       " 'core',\n",
       " 'corps',\n",
       " 'corrupt',\n",
       " 'countless',\n",
       " 'country',\n",
       " 'course',\n",
       " 'court',\n",
       " 'coverage',\n",
       " 'craft',\n",
       " 'create',\n",
       " 'created',\n",
       " 'creates',\n",
       " 'creating',\n",
       " 'creation',\n",
       " 'credit',\n",
       " 'crowns',\n",
       " 'cruelty',\n",
       " 'crunch',\n",
       " 'culmination',\n",
       " 'cure',\n",
       " 'currency',\n",
       " 'customer',\n",
       " 'cut',\n",
       " 'cutting',\n",
       " 'd-nj',\n",
       " 'd.c',\n",
       " 'day',\n",
       " 'days',\n",
       " 'deal',\n",
       " 'dean',\n",
       " 'dear',\n",
       " 'death',\n",
       " 'debbie',\n",
       " 'december',\n",
       " 'decision',\n",
       " 'decisions',\n",
       " 'dedication',\n",
       " 'defense',\n",
       " 'deforest',\n",
       " 'delays',\n",
       " 'deliver',\n",
       " 'delivery',\n",
       " 'demand',\n",
       " 'democratic',\n",
       " 'democrats',\n",
       " 'denied',\n",
       " 'dental',\n",
       " 'dentistry',\n",
       " 'dentists',\n",
       " 'deny',\n",
       " 'department',\n",
       " 'departments',\n",
       " 'depend',\n",
       " 'deserve',\n",
       " 'deserves',\n",
       " 'design',\n",
       " 'designated',\n",
       " 'designed',\n",
       " 'designs',\n",
       " 'desire',\n",
       " 'despite',\n",
       " 'destroyed',\n",
       " 'detention',\n",
       " 'determination',\n",
       " 'determined',\n",
       " 'development',\n",
       " 'devised',\n",
       " 'diabetes',\n",
       " 'different',\n",
       " 'dignity',\n",
       " 'diplomatic',\n",
       " 'direct',\n",
       " 'direction',\n",
       " 'directors',\n",
       " 'dirty',\n",
       " 'disabled',\n",
       " 'disappointment',\n",
       " 'discovery',\n",
       " 'discuss',\n",
       " 'discussion',\n",
       " 'disease',\n",
       " 'diseases',\n",
       " 'district',\n",
       " 'diverse',\n",
       " 'diversity',\n",
       " 'dodd-frank',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'dollar',\n",
       " 'dollars',\n",
       " 'don',\n",
       " 'downturn',\n",
       " 'dream',\n",
       " 'drop',\n",
       " 'drowning',\n",
       " 'earlier',\n",
       " 'earmarks',\n",
       " 'earn',\n",
       " 'earnestly',\n",
       " 'ease',\n",
       " 'easier',\n",
       " 'easing',\n",
       " 'east',\n",
       " 'eat',\n",
       " 'economic',\n",
       " 'economies',\n",
       " 'economy',\n",
       " 'education',\n",
       " 'educational',\n",
       " 'effectively',\n",
       " 'efforts',\n",
       " 'elderly',\n",
       " 'electronically',\n",
       " 'eligible',\n",
       " 'eliminating',\n",
       " 'embargo',\n",
       " 'emergencies',\n",
       " 'emergency',\n",
       " 'employees',\n",
       " 'empowered',\n",
       " 'enactment',\n",
       " 'encouraged',\n",
       " 'encourages',\n",
       " 'end',\n",
       " 'endure',\n",
       " 'energy',\n",
       " 'enforce',\n",
       " 'engaging',\n",
       " 'enhance',\n",
       " 'enormous',\n",
       " 'enriched',\n",
       " 'ensure',\n",
       " 'entire',\n",
       " 'entities',\n",
       " 'environment',\n",
       " 'equipment',\n",
       " 'equipped',\n",
       " 'era',\n",
       " 'especially',\n",
       " 'establishes',\n",
       " 'eugenia',\n",
       " 'european',\n",
       " 'event',\n",
       " 'example',\n",
       " 'exams',\n",
       " 'exchange',\n",
       " 'executive',\n",
       " 'existing',\n",
       " 'expand',\n",
       " 'expands',\n",
       " 'expel',\n",
       " 'expensing',\n",
       " 'expensive',\n",
       " 'experienced',\n",
       " 'experts',\n",
       " 'explicitly',\n",
       " 'explored',\n",
       " 'exposed',\n",
       " 'express',\n",
       " 'expressed',\n",
       " 'expressing',\n",
       " 'extractions',\n",
       " 'extraordinary',\n",
       " 'fabric',\n",
       " 'face',\n",
       " 'facilitate',\n",
       " 'facilitating',\n",
       " 'facing',\n",
       " 'factory',\n",
       " 'faith',\n",
       " 'families',\n",
       " 'family',\n",
       " 'far',\n",
       " 'fault',\n",
       " 'favor',\n",
       " 'february',\n",
       " 'federal',\n",
       " 'feldman',\n",
       " 'fema',\n",
       " 'fight',\n",
       " 'fighters',\n",
       " 'fighting',\n",
       " 'fillings',\n",
       " 'finally',\n",
       " 'financial',\n",
       " 'firefighters',\n",
       " 'fix',\n",
       " 'fleet',\n",
       " 'flourish',\n",
       " 'flow',\n",
       " 'fluoride',\n",
       " 'focus',\n",
       " 'following',\n",
       " 'foods',\n",
       " 'foreign',\n",
       " 'forth',\n",
       " 'forum',\n",
       " 'forward',\n",
       " 'fought',\n",
       " 'foundation',\n",
       " 'frank',\n",
       " 'free',\n",
       " 'freed',\n",
       " 'freedom',\n",
       " 'friday',\n",
       " 'fuel',\n",
       " 'fuels',\n",
       " 'fully',\n",
       " 'fun',\n",
       " 'fund',\n",
       " 'fundamental',\n",
       " 'funding',\n",
       " 'funds',\n",
       " 'future',\n",
       " 'gain',\n",
       " 'gambled',\n",
       " 'gardens',\n",
       " 'gas',\n",
       " 'general',\n",
       " 'generated',\n",
       " 'generating',\n",
       " 'getting',\n",
       " 'giants',\n",
       " 'gillibrand',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'goals',\n",
       " 'goes',\n",
       " 'good',\n",
       " 'goodwill',\n",
       " 'government',\n",
       " 'governments',\n",
       " 'grant',\n",
       " 'grants',\n",
       " 'great',\n",
       " 'greater',\n",
       " 'greatly',\n",
       " 'greatness',\n",
       " 'grow',\n",
       " 'grown',\n",
       " 'growth',\n",
       " 'guaranteed',\n",
       " 'guarantees',\n",
       " 'guard',\n",
       " 'guilty',\n",
       " 'hailed',\n",
       " 'hails',\n",
       " 'half',\n",
       " 'halt',\n",
       " 'hamp',\n",
       " 'handle',\n",
       " 'hard',\n",
       " 'hardship',\n",
       " 'hart',\n",
       " 'health',\n",
       " 'healthier',\n",
       " 'healthy',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'hearing',\n",
       " 'held',\n",
       " 'help',\n",
       " 'helped',\n",
       " 'helping',\n",
       " 'helps',\n",
       " 'hemisphere',\n",
       " 'highlands',\n",
       " 'hill',\n",
       " 'hinders',\n",
       " 'hiring',\n",
       " 'historically',\n",
       " 'history',\n",
       " 'homeland',\n",
       " 'homeowners',\n",
       " 'homes',\n",
       " 'hometown',\n",
       " 'honor',\n",
       " 'honored',\n",
       " 'honors',\n",
       " 'hope',\n",
       " 'hopes',\n",
       " 'host',\n",
       " 'hosted',\n",
       " 'hosts',\n",
       " 'house',\n",
       " 'housing',\n",
       " 'http',\n",
       " 'hubs',\n",
       " 'human',\n",
       " 'hurt',\n",
       " 'hygiene',\n",
       " 'hygienists',\n",
       " 'illicit',\n",
       " 'immigration',\n",
       " 'impact',\n",
       " 'importance',\n",
       " 'important',\n",
       " 'impose',\n",
       " 'imposed',\n",
       " 'imposes',\n",
       " 'imposition',\n",
       " 'impression',\n",
       " 'imprisonment',\n",
       " 'improve',\n",
       " 'improvements',\n",
       " 'incentives',\n",
       " 'include',\n",
       " 'included',\n",
       " 'including',\n",
       " 'inclusion',\n",
       " 'income',\n",
       " 'inconsistent',\n",
       " 'increase',\n",
       " 'increased',\n",
       " 'increasing',\n",
       " 'incredible',\n",
       " 'incredibly',\n",
       " 'industry',\n",
       " 'inform',\n",
       " 'information',\n",
       " 'infrastructure',\n",
       " 'inhumane',\n",
       " 'initiatives',\n",
       " 'insider',\n",
       " 'inspired',\n",
       " 'instance',\n",
       " 'instances',\n",
       " 'institute',\n",
       " 'institutions',\n",
       " 'instruction',\n",
       " 'insurance',\n",
       " 'integrity',\n",
       " 'intelligence',\n",
       " 'interbank',\n",
       " 'interests',\n",
       " 'international',\n",
       " 'internet',\n",
       " 'introduced',\n",
       " 'invest',\n",
       " 'investigate',\n",
       " 'investment',\n",
       " 'investments',\n",
       " 'investors',\n",
       " 'iran',\n",
       " 'iranian',\n",
       " 'iranian-energy',\n",
       " 'irgc',\n",
       " 'irvington',\n",
       " 'islamic',\n",
       " 'iso',\n",
       " 'issue',\n",
       " 'issued',\n",
       " 'jack',\n",
       " 'jail',\n",
       " 'jeffrey',\n",
       " 'jersey',\n",
       " 'jim',\n",
       " 'job',\n",
       " 'jobs',\n",
       " 'joined',\n",
       " 'joint',\n",
       " 'judgment',\n",
       " 'judy',\n",
       " 'just',\n",
       " 'justice',\n",
       " 'khameini',\n",
       " 'khobar',\n",
       " 'kids',\n",
       " 'killed',\n",
       " 'kirk',\n",
       " 'knowledge',\n",
       " 'komen',\n",
       " 'korea',\n",
       " 'lack',\n",
       " 'laid',\n",
       " 'landing',\n",
       " 'language',\n",
       " 'largest',\n",
       " 'later',\n",
       " 'latinas',\n",
       " 'laureldale',\n",
       " 'lautenberg',\n",
       " 'law',\n",
       " 'laws',\n",
       " 'lay',\n",
       " 'leader',\n",
       " 'leaders',\n",
       " 'learn',\n",
       " 'leases',\n",
       " 'lebanon',\n",
       " 'led',\n",
       " 'legislation',\n",
       " 'legislators',\n",
       " 'let',\n",
       " 'letter',\n",
       " 'liability',\n",
       " 'life',\n",
       " 'life-saving',\n",
       " 'lifeline',\n",
       " 'like',\n",
       " 'limits',\n",
       " 'lincoln',\n",
       " 'line',\n",
       " 'lines',\n",
       " 'literally',\n",
       " 'livable',\n",
       " 'lives',\n",
       " 'living',\n",
       " 'loan',\n",
       " 'local',\n",
       " 'logical',\n",
       " 'long',\n",
       " 'lost',\n",
       " 'low',\n",
       " 'low-income',\n",
       " 'lucky',\n",
       " 'machine',\n",
       " 'maintain',\n",
       " 'major',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'mammograms',\n",
       " 'management',\n",
       " 'mandatory',\n",
       " 'marine',\n",
       " 'mark',\n",
       " 'market',\n",
       " 'materials',\n",
       " 'mayor',\n",
       " 'mays',\n",
       " 'mean',\n",
       " 'mean-spirited',\n",
       " 'means',\n",
       " 'measures',\n",
       " 'medicaid',\n",
       " 'medical',\n",
       " 'medically',\n",
       " 'medicare',\n",
       " 'medicine',\n",
       " 'medium',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'mellat',\n",
       " 'members',\n",
       " 'men',\n",
       " 'menendez',\n",
       " 'message',\n",
       " 'messages',\n",
       " 'met',\n",
       " 'military',\n",
       " 'million',\n",
       " 'millions',\n",
       " 'minimal',\n",
       " 'minister',\n",
       " 'ministry',\n",
       " 'mission',\n",
       " 'mobile',\n",
       " 'modern',\n",
       " 'modification',\n",
       " 'money',\n",
       " 'month',\n",
       " 'moorestown',\n",
       " 'morning',\n",
       " 'mortgage',\n",
       " 'motaz',\n",
       " 'mother',\n",
       " 'motivated',\n",
       " 'multi-million',\n",
       " 'municipalities',\n",
       " 'nation',\n",
       " 'national',\n",
       " 'nationally',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'neighborhood',\n",
       " 'new',\n",
       " 'newark',\n",
       " 'news',\n",
       " 'nioc',\n",
       " 'nitc',\n",
       " 'nonprofit',\n",
       " 'nonpublic',\n",
       " 'noose',\n",
       " 'north',\n",
       " 'northern',\n",
       " 'notes',\n",
       " 'nti',\n",
       " 'nuclear',\n",
       " 'number',\n",
       " 'nutley',\n",
       " 'obama',\n",
       " 'october',\n",
       " 'offered',\n",
       " 'office',\n",
       " 'officials',\n",
       " 'oil',\n",
       " 'older',\n",
       " 'ongoing',\n",
       " 'operating',\n",
       " 'operations',\n",
       " 'opportunities',\n",
       " 'opportunity',\n",
       " 'opposition',\n",
       " 'oppression',\n",
       " 'oral',\n",
       " 'order',\n",
       " 'organizations',\n",
       " 'oriented',\n",
       " 'outside',\n",
       " 'outstanding',\n",
       " 'overall',\n",
       " 'overwhelming',\n",
       " 'owner',\n",
       " 'owners',\n",
       " 'owners–',\n",
       " 'owning',\n",
       " 'p.m',\n",
       " 'pap',\n",
       " 'paperwork',\n",
       " 'paramilitary',\n",
       " 'parenthood',\n",
       " 'parents',\n",
       " 'participated',\n",
       " 'particularly',\n",
       " 'partisan',\n",
       " 'partnered',\n",
       " 'party',\n",
       " 'passage',\n",
       " 'passed',\n",
       " 'past',\n",
       " 'pastor',\n",
       " 'path',\n",
       " 'patients',\n",
       " 'pay',\n",
       " 'paying',\n",
       " 'payment',\n",
       " 'payroll',\n",
       " 'people',\n",
       " 'percent',\n",
       " 'performed',\n",
       " 'personal',\n",
       " 'physical',\n",
       " 'pioneering',\n",
       " 'pioneers',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'planned',\n",
       " 'planning',\n",
       " 'pleased',\n",
       " 'policy',\n",
       " 'political',\n",
       " 'politically',\n",
       " 'politics',\n",
       " 'possible',\n",
       " 'post',\n",
       " 'potential',\n",
       " 'potentially',\n",
       " 'power',\n",
       " 'praised',\n",
       " 'praises',\n",
       " 'present',\n",
       " 'president',\n",
       " 'pressure',\n",
       " 'preventative',\n",
       " 'prevented',\n",
       " 'prevention',\n",
       " 'prices',\n",
       " 'primary',\n",
       " 'prime',\n",
       " 'principal',\n",
       " 'principles',\n",
       " 'prison',\n",
       " 'private',\n",
       " 'pro',\n",
       " 'pro-bono',\n",
       " 'problems',\n",
       " 'procedure',\n",
       " 'procedures',\n",
       " 'process',\n",
       " 'produced',\n",
       " 'products',\n",
       " 'profit',\n",
       " 'program',\n",
       " 'programs',\n",
       " 'prohibition',\n",
       " 'project',\n",
       " 'projects',\n",
       " 'promote',\n",
       " 'proposal',\n",
       " 'prosecute',\n",
       " 'prosecuting',\n",
       " 'prosperity',\n",
       " 'protect',\n",
       " 'proud',\n",
       " 'provide',\n",
       " 'provided',\n",
       " 'provider',\n",
       " 'providers',\n",
       " 'provides',\n",
       " 'providing',\n",
       " 'provision',\n",
       " 'provisions',\n",
       " 'public',\n",
       " 'purchase',\n",
       " 'putting',\n",
       " 'qualify',\n",
       " 'qualifying',\n",
       " 'quality',\n",
       " 'quickly',\n",
       " 'r-il',\n",
       " 'r-mi',\n",
       " 'race',\n",
       " 'raise',\n",
       " 'rank',\n",
       " 'rates',\n",
       " 'ratification',\n",
       " 'reach',\n",
       " 'read',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'reason',\n",
       " 'reauthorization',\n",
       " 'rebuild',\n",
       " 'receive',\n",
       " 'received',\n",
       " 'receiving',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'recognize',\n",
       " 'recommended',\n",
       " 'reconsider',\n",
       " 'reduce',\n",
       " 'referrals',\n",
       " 'refinancing',\n",
       " 'refocus',\n",
       " 'reformed',\n",
       " 'reforms',\n",
       " 'refrigerated',\n",
       " 'refueling',\n",
       " 'regime',\n",
       " 'regimes',\n",
       " 'regulations',\n",
       " 'regulatory',\n",
       " 'rehabilitation',\n",
       " 'reinvigorate',\n",
       " 'rejected',\n",
       " 'relations',\n",
       " 'released',\n",
       " 'releasing',\n",
       " 'relief',\n",
       " 'relieve',\n",
       " 'relieved',\n",
       " 'religion',\n",
       " 'remains',\n",
       " 'remind',\n",
       " 'reminder',\n",
       " 'renal',\n",
       " 'rendered',\n",
       " 'repair',\n",
       " 'repeal',\n",
       " 'repeating',\n",
       " 'replicated',\n",
       " 'report',\n",
       " 'reporting',\n",
       " 'representative',\n",
       " 'reputation',\n",
       " 'requirement',\n",
       " 'requires',\n",
       " 'rescue',\n",
       " 'reserves',\n",
       " 'resources',\n",
       " 'respect',\n",
       " 'respecting',\n",
       " 'respond',\n",
       " 'responders',\n",
       " 'response',\n",
       " 'responsible',\n",
       " 'restore',\n",
       " 'restoring',\n",
       " 'restrictions',\n",
       " 'resulted',\n",
       " 'results',\n",
       " 'retirements',\n",
       " 'rev',\n",
       " 'revenue',\n",
       " 'reversed',\n",
       " 'reversing',\n",
       " 'revolutionary',\n",
       " 'rich',\n",
       " 'ridgewood',\n",
       " 'right',\n",
       " 'rights',\n",
       " 'risking',\n",
       " 'riverton',\n",
       " 'robert',\n",
       " 'roger',\n",
       " 'roll',\n",
       " 'rooted',\n",
       " 'roughly',\n",
       " 'roundtable',\n",
       " 'russia',\n",
       " 'rutgers',\n",
       " 's.1048',\n",
       " 'saderat',\n",
       " 'safe',\n",
       " 'safely',\n",
       " 'safer',\n",
       " 'safety',\n",
       " 'said',\n",
       " 'sake',\n",
       " 'sanction',\n",
       " 'sanctions',\n",
       " 'saudi',\n",
       " 'save',\n",
       " 'saved',\n",
       " 'saving',\n",
       " 'says',\n",
       " 'sba',\n",
       " 'school',\n",
       " 'science',\n",
       " 'scores',\n",
       " 'screening',\n",
       " 'screenings',\n",
       " 'sea',\n",
       " 'second',\n",
       " 'secretary',\n",
       " 'sector',\n",
       " 'sectors',\n",
       " 'secure',\n",
       " 'securing',\n",
       " 'securities',\n",
       " ...]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at all those columns with number names! We didn't remove them in the tokenizer, so they're tokens. Do we want that? It depends! Sometimes the numbers in the text can be interesting information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try this again with tf-idf weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'01</th>\n",
       "      <th>'activist</th>\n",
       "      <th>'acts</th>\n",
       "      <th>'disappointing</th>\n",
       "      <th>'e-verify</th>\n",
       "      <th>'em</th>\n",
       "      <th>'liberty</th>\n",
       "      <th>'ll</th>\n",
       "      <th>'no</th>\n",
       "      <th>'re</th>\n",
       "      <th>...</th>\n",
       "      <th>…oil-drilling</th>\n",
       "      <th>…struggling</th>\n",
       "      <th>…tarp</th>\n",
       "      <th>…that</th>\n",
       "      <th>…the</th>\n",
       "      <th>…then</th>\n",
       "      <th>…there</th>\n",
       "      <th>…these</th>\n",
       "      <th>…this</th>\n",
       "      <th>…we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27660 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   '01  'activist  'acts  'disappointing  'e-verify  'em  'liberty  'll  'no  \\\n",
       "0  0.0        0.0    0.0             0.0        0.0  0.0       0.0  0.0  0.0   \n",
       "1  0.0        0.0    0.0             0.0        0.0  0.0       0.0  0.0  0.0   \n",
       "2  0.0        0.0    0.0             0.0        0.0  0.0       0.0  0.0  0.0   \n",
       "3  0.0        0.0    0.0             0.0        0.0  0.0       0.0  0.0  0.0   \n",
       "4  0.0        0.0    0.0             0.0        0.0  0.0       0.0  0.0  0.0   \n",
       "\n",
       "   're ...   …oil-drilling  …struggling  …tarp  …that  …the  …then  …there  \\\n",
       "0  0.0 ...             0.0          0.0    0.0    0.0   0.0    0.0     0.0   \n",
       "1  0.0 ...             0.0          0.0    0.0    0.0   0.0    0.0     0.0   \n",
       "2  0.0 ...             0.0          0.0    0.0    0.0   0.0    0.0     0.0   \n",
       "3  0.0 ...             0.0          0.0    0.0    0.0   0.0    0.0     0.0   \n",
       "4  0.0 ...             0.0          0.0    0.0    0.0   0.0    0.0     0.0   \n",
       "\n",
       "   …these  …this  …we  \n",
       "0     0.0    0.0  0.0  \n",
       "1     0.0    0.0  0.0  \n",
       "2     0.0    0.0  0.0  \n",
       "3     0.0    0.0  0.0  \n",
       "4     0.0    0.0  0.0  \n",
       "\n",
       "[5 rows x 27660 columns]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', tokenizer=tokenize)\n",
    "\n",
    "matrix = vectorizer.fit_transform(pr.text)\n",
    "\n",
    "# The easiest way to see what happenned is to make a dataframe\n",
    "tfidf_vectors = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "tfidf_vectors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the top words in the press release again. The stop words have gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nti', 0.56487288350283282)\n",
      "('transit', 0.30992020013592608)\n",
      "('transportation', 0.23490806362768965)\n",
      "('training', 0.20020090792648149)\n",
      "('rutgers', 0.16379220882186274)\n",
      "('4.3', 0.15701624958670632)\n",
      "('institute', 0.15030236459784593)\n",
      "('public', 0.13468667783892418)\n",
      "('efficient', 0.13247842020300446)\n",
      "('workers', 0.11513111737966622)\n",
      "('grant', 0.099558443055257573)\n",
      "('award', 0.098396096503811856)\n",
      "('critical', 0.095532368614887939)\n",
      "('larrousse.nti', 0.091692332362515894)\n",
      "('premiere', 0.086826433625999941)\n",
      "('industry', 0.083008913035465287)\n",
      "('lautenberg', 0.081471107009427737)\n",
      "('interconnected', 0.080696126214690411)\n",
      "('element', 0.080696126214690411)\n",
      "('1991', 0.080696126214690411)\n"
     ]
    }
   ],
   "source": [
    "print_sorted_vector(tfidf_vectors.iloc[212,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comapre this to the headline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence(\"Menendez and Lautenberg Applaud USDOT’s $4.3 Million Award to National Transit Institute at Rutgers\n",
       "                    \n",
       "                            Funding will provide job training and education for public transportation workers\n",
       "                    \n",
       "                      August 3, 2011\n",
       "                     WASHINGTON, D.C. – Today, U.S.\")"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
